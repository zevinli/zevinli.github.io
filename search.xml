<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>第一篇文章 First Article</title>
      <link href="/2020/03/14/first-article/"/>
      <url>/2020/03/14/first-article/</url>
      
        <content type="html"><![CDATA[<p> 欢迎来到我的博客，在这里我会分享我的日常，也会分享实用的知识。<br> 欢迎到访我的网站，也欢迎您的关注和支持。</p><p>下面是我的自我介绍：<br>沈阳航空航天大学机电工程学院本科生<br>在校学习成绩优异，GPA3.8/4.0，获综合二等奖学金，国家励志奖学金<br>学生会活动部成员，负责剪辑视频，内容输出，后台控制等工作<br>校辩论队成员，参与多次辩论比赛，并作为领队指导院内辩论赛</p><p>东北大学软件工程二学历在修<br>通过4/9考试</p><p>网易每颗豆终生学习社区第一期成员，有道精品课python课程答疑助教，CSDN博主<br>在有道python学习中获得老师赞许，在近6千人中选取80人中有幸入选，并成为网易有道课程指定python答疑助教，负责答疑python基础，网站开发，爬虫，游戏开发，人工智能，树莓派等内容，于2020年2月14日入驻CSDN，输出高质量编程内容，并签约讲师。</p><p>青媒计划第十期小组长，优秀学员，第十一期班长<br>网易青媒计划第十期排名89/8000+，自媒体运营期间b站粉丝1.2万，网易客户端获赞过万，周最热讲讲红人，知乎粉丝3千，并作为班长运营十一期。</p><p>我爱竞赛网大使，百度百家号校园大使<br>对接学校和机构桥梁，组办多次大型活动，统筹运营相关事宜。</p><p>三节课CEO助理，校园合伙人，线上实习生<br>对接三节课平台，作为CEO助理，线上实习生，实习周期6个月，圆满完成任务，并成为指定校园合伙人</p><p>刺猬实习公号运营实习生<br>负责刺猬实习的知乎方向公号运营，主要撰写文章和回答。</p><p>国际PFM高级金融管理师<br>于2019年8月完成PFM国际认证</p><p>国际科技创新大会志愿者<br>于2019年10月参加SLUSH国际创新大会，服务周期三天，志愿者中唯一对接SLUSH内部公司的人，协助完成系列工作。</p><p>C语言能力北京大学认证，人工智能协会会员，Linux运维基础阿里云认证,机器学习aideeping认证</p><p>青年创业者<br>拥有校内孵化器项目，多次承接校内合作和商业合作，摄影，后期，竞赛三项合一。</p><p>经济独立者<br>自大一下实现经济独立，拥有自己的收入，独立花销</p><p>Steam鉴赏家</p><p>高创tedx制作者组长</p><p>长河诗刊签约作家</p><p>个人奖项<br>全国中小学生经典阅读行动省级二等奖<br>发明专利一项，实用新型专利二项，软著一项，省级论文一篇<br>创新杯未来飞行器设计大赛省三等奖 机械设计大赛校一 校三 *2 物理学术竞赛校一<br>省级大创结题一项<br>第六届大学生心灵成长故事演讲比赛校三 国防军事演讲优秀奖<br>第六届大学生心灵成长故事心理征文校三 历史剧制作校二等奖<br>帮扶优秀奖 全国摄影大赛优秀奖<br>全国艾滋病知识大赛优秀奖 全国环保知识竞赛优秀奖等</p><p>爬虫技能：<br>selenium自动化爬取淘宝商品信息<br>Scrapy 框架爬取网易新闻<br>豆瓣读书爬虫，豆瓣读书SQL ALCHEMY入库<br>模拟登陆 GitHub, GitHub 本地文件存储，快代理代理池构建<br>知乎热榜异步网络爬虫<br>分布式爬取QQ音乐歌手信息，分布式下载QQ音乐并入库MYSQL<br>多进程多线程爬取房天下并入库MYSQL<br>Celery 分布式爬取京东网商品信息</p><p>◆精通 Python语言，熟悉Linux系统操作，熟悉MySQL数据库<br>◆精通 Requests库、BeautifulSoup 库、xpath，Scrapy 框架等<br>◆掌握 Fladder抓包工具,掌握计算机网络基础，掌握Python四大HTML解析库，掌握正则表达式用法，掌握多线程与多进程爬虫技术，掌握Celery分布式爬虫技术，掌握GitHub基本用法<br>◆掌握关系型数据库 MySQL用法，熟悉使用Ajax实现目标图片下载<br>◆熟悉使用 NumPy库进行数值计算、使用Matplotlib库将数据可视化、<br>使用Pandas库进行统计分析和数据与处理、使用scikit-learn构建模型</p>]]></content>
      
      
      <categories>
          
          <category> -声明 -博主简介 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -个人相关 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我要做些什么 How I Will Do？</title>
      <link href="/2020/03/14/title/"/>
      <url>/2020/03/14/title/</url>
      
        <content type="html"><![CDATA[<p>因为个人比较喜欢记录生活，会在此分享我的经历，我的一切学习的内容和一些关于世界万物万事的独到的见解，当然也包括我的一些不成熟的想法观点和思路，包括我学习路上的一切一切。</p><p>本年我会在matlab，python，神经网络，机械原理上侧重。</p><p>附上：</p><p>新开始，新起点，告别过去，憧憬未来。<br>                          一生很长，余生很短。<br>无论是自在超然，还是想扬名立万，都应该将信念寄托彼岸。可于高台煮茶论道，也可于职场笑谈风声，三两杯小酒，道尽天下之事，黑白交错二棋，尽展浩然正气。进可为天下生民立命，退可柴米油盐酱醋茶。可与人语心中之事，亦可平天下不平之事。情商不高，不多不少，做人无碍。智商不够，努力不少，天道酬勤。<br>人生就是要做到，每时每刻都有属于自己的那一份精彩。<br>以前的我，虽说展示出看破红尘姻缘，居高旁观，亦善良温柔，带两三分偏执。<br>但从现在起，要做一个真正优秀的人，不再把机会拱手相让，把每一天的生活当为享受，无论是工作学习还是玩，都有分寸，充满动力，在现有能力基础上，多加培养全面能力，主动出击。</p><p>是差点运气，可我一直在努力。<br>不要任何借口，不需要任何理由。</p><p>秀雅两三分，展温情尔雅。<br>豪迈四两半，为凌云壮志。</p><p>说不上是内向还是外向，也谈不明是理性还是感性，就是这样一个人，他可能为了你，倾尽所有，甚至放弃天下，也可能为了天下，放弃你。</p><p>面对离别也会感伤，但过后又是微笑模样。<br>面对不解欺凌也会默默承受，不语，埋头哭泣。</p><p>酒过三旬最无奈，语言三分最伤情。<br>是好是坏，自己评判，不由分说，不闻不问。</p><p>18岁的我，<br>三岁的内心年龄，也会撒娇，也会固执，开朗无比，不丢善良，纯真无比。<br>100岁的外在年龄，也会敷衍，也会隐瞒，波澜不惊，不缺镇定，掌控局势。</p><p>文学不差，读书过千，速读精读都会，古典文学，现代科幻，言情小说，现代文学，悬疑小说，人物传记，无所不看。爱写书评，深度解析内容。常暑文记录生活点滴，亦作诗作曲。</p><p>精于科学，热衷于高科技，尤其是量子物理，科学杂志常年订阅者，（大科技，sciences，nature，科学fans等）一个为了科学发疯的疯子。</p><p>哲学入门，思辨能力，推演能力都还在行，也懂法学，系统学习过心理学，曾就读于国际nlp导师课程。对于潜能开发也有一定接触。</p><p>精通于计算机，了解计算机原理并且熟悉计算机组装等。MATLAB，PHP，前端等 尤其擅长Python编程，神经网络，树莓派，在爬虫，网站开发，游戏开发，大数据分析都有所擅长，现任有道每科豆助教。</p><p>office，Adobe熟练应用不成问题，c语言，c++也还行。擅长后期制作，但仍然需要学习进步，曾参与历史剧制作获得10强称号。b站粉丝1.2万。</p><p>会古琴，吉他，笛子【正在学】三种乐器，也想接触黑管这门西方乐器，偶尔也喜欢ktv纵情歌唱。</p><p>擅长朗诵，主持，讲课，演讲。主持团成员，主持过各大场合，朗诵在高中便小有名气，讲课逻辑深刻，让人难忘。演讲让人震撼，常常能打动人心，触动人的心灵的那份善良。</p><p>围棋高手，亦会象棋跳棋各种棋。没事可以交流交流。</p><p>美食缔造家，居家好男人。我不会告诉你的。</p><p>会各种球类，虽然大多数玩的很菜，但还是可以玩一玩，最喜欢羽毛球了，可大学了，只能踢踢足球，打打篮球，偶然游个泳，打个台球，可约一波哟。</p><p>柔韧性天赋极高，不练舞蹈可惜了，我会考虑学街舞。</p><p>热爱电影，喜爱各种电影，影评高产者，能从独到视角解析影片。</p><p>热爱音乐，各种音乐都喜欢，纯音乐，电音，流行，爵士，欧美无所不爱。【尤爱纯音乐】歌手喜爱许嵩，十年不改。也喜欢徐秉龙。</p><p>绘画爱好者，8年业余，板绘初入门。</p><p>地图一样的感知，走一次便能记得路途。喜爱游玩散步，常常散步于自然。</p><p>喜爱各种竞赛，爱创造发明些小物件，常常有别人想不到的创意和点子，曾被称为鬼才逻辑师。目前有一发明，二实用新型专利，一篇省级论文，一软著，数学建模，机械设计大赛均获奖，互联网+进入省赛，创新杯省三，物理学术进入省赛，20余。【我会努力的！】</p><p>金融行业学习者，目前持有国际pfm证书，准备acca考试ing。</p><p>自媒体爱好者，网易青媒计划第十期小组长，优秀学员，学习期间获赞高达1万，获得最热讲讲红人称号。十一期班长。</p><p>刺猬实习社群运营实习生。<br>三节课CEO助理实习生，三节课指定校园合伙人。<br>任我爱竞赛网校园大使，百度百家号校园大使，百家号科学一班学员。</p><p>摄影爱好者，擅长摄影，剪辑，与pspr结合，所拍摄照片曾获全国摄影竞赛优秀奖。</p><p>金融独立者，大二上开学以后靠实习，奖学金，工作室收入完全经济独立。</p><p>2020年，要更努力鸭</p>]]></content>
      
      
      <categories>
          
          <category> -编程 -计算机 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -个人相关 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网路 How to distinguish a cat</title>
      <link href="/2020/03/14/hello-world/"/>
      <url>/2020/03/14/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Logistic Regression with a Neural Network mindset</p><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p>Instructions:<br>•Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</p><p>You will learn to:<br>•Build the general architecture of a learning algorithm, including: ◦Initializing parameters<br>◦Calculating the cost function and its gradient<br>◦Using an optimization algorithm (gradient descent)</p><p>•Gather all three functions above into a main model function, in the right order.</p><p>Updates</p><p>This notebook has been updated over the past few months. The prior version was named “v5”, and the current versionis now named ‘6a’</p><p>If you were working on a previous version:<br>•You can find your prior work by looking in the file directory for the older files (named by version name).<br>•To view the file directory, click on the “Coursera” icon in the top left corner of this notebook.<br>•Please copy your work from the older versions to the new version, in order to submit your work for grading.</p><p>List of Updates<br>•Forward propagation formula, indexing now starts at 1 instead of 0.<br>•Optimization function comment now says “print cost every 100 training iterations” instead of “examples”.<br>•Fixed grammar in the comments.<br>•Y_prediction_test variable name is used consistently.<br>•Plot’s axis label now says “iterations (hundred)” instead of “iterations”.<br>•When testing the model, the test image is normalized by dividing by 255.</p><p>1 - Packages</p><p>First, let’s run the cell below to import all the packages that you will need during this assignment.<br>•numpy is the fundamental package for scientific computing with Python.<br>•h5py is a common package to interact with a dataset that is stored on an H5 file.<br>•matplotlib is a famous library to plot graphs in Python.<br>•PIL and scipy are used here to test your model with your own picture at the end.<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import h5py<br>import scipy<br>from PIL import Image<br>from scipy import ndimage<br>from lr_utils import load_dataset</p><p>%matplotlib inline</p><p>2 - Overview of the Problem set</p><p>Problem Statement: You are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</p><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><h1 id="Loading-the-data-cat-non-cat"><a href="#Loading-the-data-cat-non-cat" class="headerlink" title="Loading the data (cat/non-cat)"></a>Loading the data (cat/non-cat)</h1><p>train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</p><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the  index  value and re-run to see other images.</p><h1 id="Example-of-a-picture"><a href="#Example-of-a-picture" class="headerlink" title="Example of a picture"></a>Example of a picture</h1><p>index = 25<br>plt.imshow(train_set_x_orig[index])<br>print (“y = “ + str(train_set_y[:, index]) + “, it’s a ‘“ + classes[np.squeeze(train_set_y[:, index])].decode(“utf-8”) +  “‘ picture.”)</p><p>y = [1], it’s a ‘cat’ picture.</p><p>png</p><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p><p>Exercise: Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that  train_set_x_orig  is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access  m_train  by writing  train_set_x_orig.shape[0] .</p><h3 id="START-CODE-HERE-≈-3-lines-of-code"><a href="#START-CODE-HERE-≈-3-lines-of-code" class="headerlink" title="START CODE HERE ### (≈ 3 lines of code)"></a>START CODE HERE ### (≈ 3 lines of code)</h3><p>m_train = train_set_x_orig.shape[0]<br>m_test = test_set_x_orig.shape[0]<br>num_px = train_set_x_orig.shape[1]</p><h3 id="END-CODE-HERE"><a href="#END-CODE-HERE" class="headerlink" title="END CODE HERE"></a>END CODE HERE</h3><p>print (“Number of training examples: m_train = “ + str(m_train))<br>print (“Number of testing examples: m_test = “ + str(m_test))<br>print (“Height/Width of each image: num_px = “ + str(num_px))<br>print (“Each image is of size: (“ + str(num_px) + “, “ + str(num_px) + “, 3)”)<br>print (“train_set_x shape: “ + str(train_set_x_orig.shape))<br>print (“train_set_y shape: “ + str(train_set_y.shape))<br>print (“test_set_x shape: “ + str(test_set_x_orig.shape))<br>print (“test_set_y shape: “ + str(test_set_y.shape))</p><p>Number of training examples: m_train = 209<br>Number of testing examples: m_test = 50<br>Height/Width of each image: num_px = 64<br>Each image is of size: (64, 64, 3)<br>train_set_x shape: (209, 64, 64, 3)<br>train_set_y shape: (1, 209)<br>test_set_x shape: (50, 64, 64, 3)<br>test_set_y shape: (1, 50)</p><p>Expected Output for m_train, m_test and num_px:</p><p><strong>m_train</strong> 209<br><strong>m_test</strong> 50<br><strong>num_px</strong> 64  </p><p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $$ num_px $$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p><p>Exercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $$ num_px $$ 3, 1).</p><p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$$c$$d, a) is to use:<br>X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X</p><h1 id="Reshape-the-training-and-test-examples"><a href="#Reshape-the-training-and-test-examples" class="headerlink" title="Reshape the training and test examples"></a>Reshape the training and test examples</h1><h3 id="START-CODE-HERE-≈-2-lines-of-code"><a href="#START-CODE-HERE-≈-2-lines-of-code" class="headerlink" title="START CODE HERE ### (≈ 2 lines of code)"></a>START CODE HERE ### (≈ 2 lines of code)</h3><p>train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T<br>test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T</p><h3 id="END-CODE-HERE-1"><a href="#END-CODE-HERE-1" class="headerlink" title="END CODE HERE"></a>END CODE HERE</h3><p>print (“train_set_x_flatten shape: “ + str(train_set_x_flatten.shape))<br>print (“train_set_y shape: “ + str(train_set_y.shape))<br>print (“test_set_x_flatten shape: “ + str(test_set_x_flatten.shape))<br>print (“test_set_y shape: “ + str(test_set_y.shape))<br>print (“sanity check after reshaping: “ + str(train_set_x_flatten[0:5,0]))</p><p>train_set_x_flatten shape: (12288, 209)<br>train_set_y shape: (1, 209)<br>test_set_x_flatten shape: (12288, 50)<br>test_set_y shape: (1, 50)<br>sanity check after reshaping: [17 31 56 22 33]</p><p>Expected Output:</p><p><strong>train_set_x_flatten shape</strong> (12288, 209)<br><strong>train_set_y shape</strong> (1, 209)<br><strong>test_set_x_flatten shape</strong> (12288, 50)<br><strong>test_set_y shape</strong> (1, 50)<br><strong>sanity check after reshaping</strong> [17 31 56 22 33] </p><p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p><p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p><p>Let’s standardize our dataset.<br>train_set_x = train_set_x_flatten/255.<br>test_set_x = test_set_x_flatten/255.</p><p> <strong>What you need to remember:</strong><br>Common steps for pre-processing a new dataset are:<br>•Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)<br>•Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)<br>•”Standardize” the data</p><p>3 - General Architecture of the learning algorithm</p><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!</p><p>Mathematical expression of the algorithm:</p><p>For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b \tag{1}$$ $$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$ $$ \mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} ) \log(1-a^{(i)})\tag{3}$$</p><p>The cost is then computed by summing over all training examples: $$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$</p><p>Key steps: In this exercise, you will carry out the following steps: - Initialize the parameters of the model - Learn the parameters for the model by minimizing the cost</p><ul><li>Use the learned parameters to make predictions (on the test set) - Analyse the results and conclude</li></ul><p>4 - Building the parts of our algorithm</p><p>The main steps for building a Neural Network are:<br>1.Define the model structure (such as number of input features)<br>2.Initialize the model’s parameters<br>3.Loop: ◦Calculate current loss (forward propagation)<br>◦Calculate current gradient (backward propagation)<br>◦Update parameters (gradient descent)</p><p>You often build 1-3 separately and integrate them into one function we call  model() .</p><p>4.1 - Helper functions</p><p>Exercise: Using your code from “Python Basics”, implement  sigmoid() . As you’ve seen in the figure above, you need to compute $sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp().</p><h1 id="GRADED-FUNCTION-sigmoid"><a href="#GRADED-FUNCTION-sigmoid" class="headerlink" title="GRADED FUNCTION: sigmoid"></a>GRADED FUNCTION: sigmoid</h1><p>def sigmoid(z):<br>    “””<br>    Compute the sigmoid of z</p><pre><code>Arguments:z -- A scalar or numpy array of any size.Return:s -- sigmoid(z)&quot;&quot;&quot;### START CODE HERE ### (≈ 1 line of code)s = 1.0/(1+np.exp(-z))### END CODE HERE ###return s</code></pre><p>print (“sigmoid([0, 2]) = “ + str(sigmoid(np.array([0,2]))))</p><p>sigmoid([0, 2]) = [ 0.5         0.88079708]</p><p>Expected Output:</p><p><strong>sigmoid([0, 2])</strong> [ 0.5 0.88079708] </p><p>4.2 - Initializing parameters</p><p>Exercise: Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.</p><h1 id="GRADED-FUNCTION-initialize-with-zeros"><a href="#GRADED-FUNCTION-initialize-with-zeros" class="headerlink" title="GRADED FUNCTION: initialize_with_zeros"></a>GRADED FUNCTION: initialize_with_zeros</h1><p>def initialize_with_zeros(dim):<br>    “””<br>    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</p><pre><code>Argument:dim -- size of the w vector we want (or number of parameters in this case)Returns:w -- initialized vector of shape (dim, 1)b -- initialized scalar (corresponds to the bias)&quot;&quot;&quot;### START CODE HERE ### (≈ 1 line of code)w = np.zeros((dim, 1))b = 0### END CODE HERE ###assert(w.shape == (dim, 1))assert(isinstance(b, float) or isinstance(b, int))return w, b</code></pre><p>dim = 2<br>w, b = initialize_with_zeros(dim)<br>print (“w = “ + str(w))<br>print (“b = “ + str(b))</p><p>w = [[ 0.]<br> [ 0.]]<br>b = 0</p><p>Expected Output:</p><p>** w **  [[ 0.][ 0.]]<br>** b **  0  </p><p>For image inputs, w will be of shape (num_px $\times$ num_px $\times$ 3, 1).</p><p>4.3 - Forward and Backward propagation</p><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p>Exercise: Implement a function  propagate()  that computes the cost function and its gradient.</p><p>Hints:</p><p>Forward Propagation:<br>•You get X<br>•You compute $A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, …, a^{(m-1)}, a^{(m)})$<br>•You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</p><p>Here are the two formulas you will be using:</p><p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$ $$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p><h1 id="GRADED-FUNCTION-propagate"><a href="#GRADED-FUNCTION-propagate" class="headerlink" title="GRADED FUNCTION: propagate"></a>GRADED FUNCTION: propagate</h1><p>def propagate(w, b, X, Y):<br>    “””<br>    Implement the cost function and its gradient for the propagation explained above</p><pre><code>Arguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of size (num_px * num_px * 3, number of examples)Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)Return:cost -- negative log-likelihood cost for logistic regressiondw -- gradient of the loss with respect to w, thus same shape as wdb -- gradient of the loss with respect to b, thus same shape as bTips:- Write your code step by step for the propagation. np.log(), np.dot()&quot;&quot;&quot;m = X.shape[1]# FORWARD PROPAGATION (FROM X TO COST)### START CODE HERE ### (≈ 2 lines of code)A = sigmoid(np.dot(w.T, X)+b)                                    # compute activationcost = -(1.0/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                 # compute cost### END CODE HERE #### BACKWARD PROPAGATION (TO FIND GRAD)### START CODE HERE ### (≈ 2 lines of code)dw = (1.0/m)*np.dot(X,(A-Y).T)db = (1.0/m)*np.sum(A-Y)### END CODE HERE ###assert(dw.shape == w.shape)assert(db.dtype == float)cost = np.squeeze(cost)assert(cost.shape == ())grads = {&quot;dw&quot;: dw,         &quot;db&quot;: db}return grads, cost</code></pre><p>w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])<br>grads, cost = propagate(w, b, X, Y)<br>print (“dw = “ + str(grads[“dw”]))<br>print (“db = “ + str(grads[“db”]))<br>print (“cost = “ + str(cost))</p><p>dw = [[ 0.99845601]<br> [ 2.39507239]]<br>db = 0.00145557813678<br>cost = 5.80154531939</p><p>Expected Output:</p><p>** dw **  [[ 0.99845601][ 2.39507239]]<br>** db **  0.00145557813678<br>** cost **  5.801545319394553  </p><p>4.4 - Optimization<br>•You have initialized your parameters.<br>•You are also able to compute a cost function and its gradient.<br>•Now, you want to update the parameters using gradient descent.</p><p>Exercise: Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $ \theta = \theta - \alpha \text{ } d\theta$, where $\alpha$ is the learning rate.</p><h1 id="GRADED-FUNCTION-optimize"><a href="#GRADED-FUNCTION-optimize" class="headerlink" title="GRADED FUNCTION: optimize"></a>GRADED FUNCTION: optimize</h1><p>def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):<br>    “””<br>    This function optimizes w and b by running a gradient descent algorithm</p><pre><code>Arguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of shape (num_px * num_px * 3, number of examples)Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)num_iterations -- number of iterations of the optimization looplearning_rate -- learning rate of the gradient descent update ruleprint_cost -- True to print the loss every 100 stepsReturns:params -- dictionary containing the weights w and bias bgrads -- dictionary containing the gradients of the weights and bias with respect to the cost functioncosts -- list of all the costs computed during the optimization, this will be used to plot the learning curve.Tips:You basically need to write down two steps and iterate through them:    1) Calculate the cost and the gradient for the current parameters. Use propagate().    2) Update the parameters using gradient descent rule for w and b.&quot;&quot;&quot;costs = []for i in range(num_iterations):    # Cost and gradient calculation (≈ 1-4 lines of code)    ### START CODE HERE ###     grads, cost = propagate(w, b, X, Y)    ### END CODE HERE ###    # Retrieve derivatives from grads    dw = grads[&quot;dw&quot;]    db = grads[&quot;db&quot;]    # update rule (≈ 2 lines of code)    ### START CODE HERE ###    w = w - learning_rate*dw    b = b - learning_rate*db    ### END CODE HERE ###    # Record the costs    if i % 100 == 0:        costs.append(cost)    # Print the cost every 100 training examples    if print_cost and i % 100 == 0:        print (&quot;Cost after iteration %i: %f&quot; %(i, cost))params = {&quot;w&quot;: w,          &quot;b&quot;: b}grads = {&quot;dw&quot;: dw,         &quot;db&quot;: db}return params, grads, costs</code></pre><p>params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)</p><p>print (“w = “ + str(params[“w”]))<br>print (“b = “ + str(params[“b”]))<br>print (“dw = “ + str(grads[“dw”]))<br>print (“db = “ + str(grads[“db”]))</p><p>w = [[ 0.19033591]<br> [ 0.12259159]]<br>b = 1.92535983008<br>dw = [[ 0.67752042]<br> [ 1.41625495]]<br>db = 0.219194504541</p><p>Expected Output:</p><tr>   <td> **b** </td>   <td> 1.92535983008 </td></tr><tr>   <td> **dw** </td>   <td> [[ 0.67752042]<p>[ 1.41625495]] </p><p><strong>w</strong>  [[ 0.19033591][ 0.12259159]]<br>db  0.219194504541  </p><p>Exercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the  predict()  function. There are two steps to computing predictions:</p><p>1.Calculate $\hat{Y} = A = \sigma(w^T X + b)$</p><p>2.Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector  Y_prediction . If you wish, you can use an  if / else  statement in a  for  loop (though there is also a way to vectorize this).</p><h1 id="GRADED-FUNCTION-predict"><a href="#GRADED-FUNCTION-predict" class="headerlink" title="GRADED FUNCTION: predict"></a>GRADED FUNCTION: predict</h1><p>def predict(w, b, X):<br>    ‘’’<br>    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</p><pre><code>Arguments:w -- weights, a numpy array of size (num_px * num_px * 3, 1)b -- bias, a scalarX -- data of size (num_px * num_px * 3, number of examples)Returns:Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X&apos;&apos;&apos;m = X.shape[1]Y_prediction = np.zeros((1,m))w = w.reshape(X.shape[0], 1)# Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture### START CODE HERE ### (≈ 1 line of code)A = sigmoid(np.dot(w.T, X) + b)### END CODE HERE ###for i in range(A.shape[1]):    # Convert probabilities A[0,i] to actual predictions p[0,i]    ### START CODE HERE ### (≈ 4 lines of code)    if A[0,i] &gt; 0.5:        Y_prediction[0,i] = 1    else:        Y_prediction[0,i] = 0    ### END CODE HERE ###assert(Y_prediction.shape == (1, m))return Y_prediction</code></pre><p>w = np.array([[0.1124579],[0.23106775]])<br>b = -0.3<br>X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])<br>print (“predictions = “ + str(predict(w, b, X)))</p><p>predictions = [[ 1.  1.  0.]]</p><p>Expected Output:</p><p><strong>predictions</strong>  [[ 1. 1. 0.]]<br> <strong>What to remember:</strong> You’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b):- computing the cost and its gradient- updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples<br>5 - Merge all functions into a model</p><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p>Exercise: Implement the model function. Use the following notation: - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - w, costs, grads for the outputs of optimize()</p><h1 id="GRADED-FUNCTION-model"><a href="#GRADED-FUNCTION-model" class="headerlink" title="GRADED FUNCTION: model"></a>GRADED FUNCTION: model</h1><p>def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):<br>    “””<br>    Builds the logistic regression model by calling the function you’ve implemented previously</p><pre><code>Arguments:X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)num_iterations -- hyperparameter representing the number of iterations to optimize the parameterslearning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()print_cost -- Set to true to print the cost every 100 iterationsReturns:d -- dictionary containing information about the model.&quot;&quot;&quot;### START CODE HERE #### initialize parameters with zeros (≈ 1 line of code)w, b = initialize_with_zeros(X_train.shape[0])# Gradient descent (≈ 1 line of code)parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)# Retrieve parameters w and b from dictionary &quot;parameters&quot;w = parameters[&quot;w&quot;]b = parameters[&quot;b&quot;]# Predict test/train set examples (≈ 2 lines of code)Y_prediction_test = predict(w, b, X_test)Y_prediction_train = predict(w, b, X_train)### END CODE HERE #### Print train/test Errorsprint(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))d = {&quot;costs&quot;: costs,     &quot;Y_prediction_test&quot;: Y_prediction_test,      &quot;Y_prediction_train&quot; : Y_prediction_train,      &quot;w&quot; : w,      &quot;b&quot; : b,     &quot;learning_rate&quot; : learning_rate,     &quot;num_iterations&quot;: num_iterations}return d</code></pre><p>Run the following cell to train your model.<br>d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)</p><p>Cost after iteration 0: 0.693147<br>Cost after iteration 100: 0.584508<br>Cost after iteration 200: 0.466949<br>Cost after iteration 300: 0.376007<br>Cost after iteration 400: 0.331463<br>Cost after iteration 500: 0.303273<br>Cost after iteration 600: 0.279880<br>Cost after iteration 700: 0.260042<br>Cost after iteration 800: 0.242941<br>Cost after iteration 900: 0.228004<br>Cost after iteration 1000: 0.214820<br>Cost after iteration 1100: 0.203078<br>Cost after iteration 1200: 0.192544<br>Cost after iteration 1300: 0.183033<br>Cost after iteration 1400: 0.174399<br>Cost after iteration 1500: 0.166521<br>Cost after iteration 1600: 0.159305<br>Cost after iteration 1700: 0.152667<br>Cost after iteration 1800: 0.146542<br>Cost after iteration 1900: 0.140872<br>train accuracy: 99.04306220095694 %<br>test accuracy: 70.0 %</p><p>Expected Output:</p></td></tr><tr>    <td> **Cost after iteration 0 **  </td>     <td> 0.693147 </td></tr>  <tr>    <td> <center> $\vdots$ </center> </td>     <td> <center> $\vdots$ </center> </td> </tr>  <tr>    <td> **Train Accuracy**  </td>     <td> 99.04306220095694 % </td></tr><tr>    <td>**Test Accuracy** </td>     <td> 70.0 % </td></tr><p>Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the  index  variable) you can look at predictions on pictures of the test set.</p><h1 id="Example-of-a-picture-that-was-wrongly-classified"><a href="#Example-of-a-picture-that-was-wrongly-classified" class="headerlink" title="Example of a picture that was wrongly classified."></a>Example of a picture that was wrongly classified.</h1><p>index = 1<br>plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))<br>print (“y = “ + str(test_set_y[0,index]) + “, you predicted that it is a &quot;“ + classes[d[“Y_prediction_test”][0,index]].decode(“utf-8”) +  “&quot; picture.”)</p><p>y = 1, you predicted that it is a “cat” picture.</p><p>png</p><p>Let’s also plot the cost function and the gradients.</p><h1 id="Plot-learning-curve-with-costs"><a href="#Plot-learning-curve-with-costs" class="headerlink" title="Plot learning curve (with costs)"></a>Plot learning curve (with costs)</h1><p>costs = np.squeeze(d[‘costs’])<br>plt.plot(costs)<br>plt.ylabel(‘cost’)<br>plt.xlabel(‘iterations (per hundreds)’)<br>plt.title(“Learning rate =” + str(d[“learning_rate”]))<br>plt.show()</p><p>png</p><p>Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p><p>6 - Further analysis (optional/ungraded exercise)</p><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate $\alpha$.</p><p>Choice of learning rate</p><p>Reminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\alpha$ determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the  learning_rates  variable to contain, and see what happens.<br>learning_rates = [0.01, 0.001, 0.0001]<br>models = {}<br>for i in learning_rates:<br>    print (“learning rate is: “ + str(i))<br>    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)<br>    print (‘\n’ + “——————————————————-“ + ‘\n’)</p><p>for i in learning_rates:<br>    plt.plot(np.squeeze(models[str(i)][“costs”]), label= str(models[str(i)][“learning_rate”]))</p><p>plt.ylabel(‘cost’)<br>plt.xlabel(‘iterations (hundreds)’)</p><p>legend = plt.legend(loc=’upper center’, shadow=True)<br>frame = legend.get_frame()<br>frame.set_facecolor(‘0.90’)<br>plt.show()</p><p>learning rate is: 0.01<br>train accuracy: 99.52153110047847 %<br>test accuracy: 68.0 %</p><hr><p>learning rate is: 0.001<br>train accuracy: 88.99521531100478 %<br>test accuracy: 64.0 %</p><hr><p>learning rate is: 0.0001<br>train accuracy: 68.42105263157895 %<br>test accuracy: 36.0 %</p><hr><p>png</p><p>Interpretation:<br>•Different learning rates give different costs and thus different predictions results.<br>•If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).<br>•A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.<br>•In deep learning, we usually recommend that you: ◦Choose the learning rate that better minimizes the cost function.<br>◦If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</p><p>7 - Test with your own image (optional/ungraded exercise)</p><p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: 1. Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook’s directory, in the “images” folder 3. Change your image’s name in the following code 4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</p><h2 id="START-CODE-HERE-PUT-YOUR-IMAGE-NAME"><a href="#START-CODE-HERE-PUT-YOUR-IMAGE-NAME" class="headerlink" title="START CODE HERE ## (PUT YOUR IMAGE NAME)"></a>START CODE HERE ## (PUT YOUR IMAGE NAME)</h2><p>my_image = “my_image.jpg”   # change this to the name of your image file </p><h2 id="END-CODE-HERE-2"><a href="#END-CODE-HERE-2" class="headerlink" title="END CODE HERE"></a>END CODE HERE</h2><h1 id="We-preprocess-the-image-to-fit-your-algorithm"><a href="#We-preprocess-the-image-to-fit-your-algorithm" class="headerlink" title="We preprocess the image to fit your algorithm."></a>We preprocess the image to fit your algorithm.</h1><p>fname = “images/“ + my_image<br>image = np.array(ndimage.imread(fname, flatten=False))<br>image = image/255.<br>my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px<em>num_px</em>3)).T<br>my_predicted_image = predict(d[“w”], d[“b”], my_image)</p><p>plt.imshow(image)<br>print(“y = “ + str(np.squeeze(my_predicted_image)) + “, your algorithm predicts a &quot;“ + classes[int(np.squeeze(my_predicted_image)),].decode(“utf-8”) +  “&quot; picture.”)</p><p>y = 0.0, your algorithm predicts a “non-cat” picture.</p><p>png<br> <strong>What to remember from this assignment:</strong> 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!<br>Finally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include: - Play with the learning rate and the number of iterations - Try different initialization methods and compare the results - Test other preprocessings (center the data, or divide each row by its standard deviation)</p><p>Bibliography:<br>•<a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a><br>•<a href="https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c</a></p>]]></content>
      
      
      <categories>
          
          <category> -声明 -博主简介 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -python -神经网路 -numpy -例题 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
